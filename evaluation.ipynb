{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "s_output = load('checkpoints/S_outputs_text.joblib')\n",
    "l_output = load('checkpoints/L_outputs_text.joblib')\n",
    "output = load('checkpoints/outputs_text.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = 'moussaKam/AraBART'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def tokenize(df):\n",
    "    model_input = [row for row in df['text']]\n",
    "    model_input = tokenizer(model_input, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(df['summary'], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_input['labels'] = labels['input_ids']\n",
    "    return model_input\n",
    "\n",
    "\n",
    "column_name = ['cleaned_text', 'Lemmatized_Text', 'Stemmed_Text']\n",
    "\n",
    "def prepare_data(column_index=0):\n",
    "\n",
    "    train_inputs = pd.read_csv('../preprocessed_data/train_inputs.csv')\n",
    "    test_inputs = pd.read_csv('../preprocessed_data/test_inputs.csv')\n",
    "    validation_inputs = pd.read_csv('../preprocessed_data/validation_inputs.csv')\n",
    "\n",
    "    train_labels = pd.read_csv('../preprocessed_data/train_labels.csv')\n",
    "    test_labels = pd.read_csv('../preprocessed_data/test_labels.csv')\n",
    "    validation_labels = pd.read_csv('../preprocessed_data/validation_labels.csv')\n",
    "\n",
    "    train_size = round(train_labels.shape[0] * 0.2)\n",
    "    test_size = round(test_labels.shape[0] * 0.2)\n",
    "    val_size = round(validation_labels.shape[0] * 0.2)\n",
    "\n",
    "    train_inputs = train_inputs.iloc[:train_size]\n",
    "    train_labels = train_labels.iloc[:train_size]\n",
    "\n",
    "    test_inputs = test_inputs.iloc[:test_size]\n",
    "    test_labels = test_labels.iloc[:test_size]\n",
    "\n",
    "    validation_inputs = validation_inputs.iloc[:val_size]\n",
    "    validation_labels = validation_labels.iloc[:val_size]\n",
    "    df_train_x = train_inputs.copy().rename(columns={column_name[column_index]: 'text'})\n",
    "    df_train_y = train_labels.copy().rename(columns={'cleaned_text': 'summary'})\n",
    "    df_train = pd.concat([df_train_x['text'], df_train_y['summary']], axis=1)\n",
    "\n",
    "    df_test_x = test_inputs.copy().rename(columns={column_name[column_index]: 'text'})\n",
    "    df_test_y = test_labels.copy().rename(columns={'cleaned_text': 'summary'})\n",
    "    df_test = pd.concat([df_test_x['text'], df_test_y['summary']], axis=1)\n",
    "\n",
    "    df_val_x = validation_inputs.copy().rename(columns={column_name[column_index]: 'text'})\n",
    "    df_val_y = validation_labels.copy().rename(columns={'cleaned_text': 'summary'})\n",
    "    df_validation = pd.concat([df_val_x['text'], df_val_y['summary']], axis=1)\n",
    "\n",
    "    train_dataset = Dataset.from_dict(df_train)\n",
    "    test_dataset = Dataset.from_dict(df_test)\n",
    "    validation_dataset = Dataset.from_dict(df_validation)\n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"validation\": validation_dataset})\n",
    "    tokenized_data = dataset.map(tokenize, batched=True)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "outputs = [output, l_output, s_output]\n",
    "\n",
    "def evaluate(tokenized_data, index=0):\n",
    "\n",
    "    score = Rouge().get_scores(outputs[index], tokenized_data['test']['summary'])\n",
    "\n",
    "    for i in range(0, 3):\n",
    "        print(\"rouge-1 :\" , score[i]['rouge-1']['f']*100)\n",
    "        print(\"rouge-2 :\" , score[i]['rouge-2']['f']*100)\n",
    "        print(\"rouge-l :\" , score[i]['rouge-l']['f']*100)\n",
    "        print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7504 [00:00<?, ? examples/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1 : 20.83333290364584\n",
      "rouge-2 : 8.163264897959204\n",
      "rouge-l : 16.66666623697918\n",
      "\n",
      "rouge-1 : 15.789473196675916\n",
      "rouge-2 : 5.405404923301723\n",
      "rouge-l : 10.52631530193908\n",
      "\n",
      "rouge-1 : 38.88888840277778\n",
      "rouge-2 : 11.42857093877553\n",
      "rouge-l : 38.88888840277778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = prepare_data(0)\n",
    "evaluate(tokenized_data, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7504 [00:00<?, ? examples/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1 : 22.72727235227273\n",
      "rouge-2 : 8.88888854320989\n",
      "rouge-l : 22.72727235227273\n",
      "\n",
      "rouge-1 : 11.111110635802488\n",
      "rouge-2 : 5.714285247346978\n",
      "rouge-l : 11.111110635802488\n",
      "\n",
      "rouge-1 : 47.05882305709343\n",
      "rouge-2 : 18.749999531250012\n",
      "rouge-l : 47.05882305709343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = prepare_data(1)\n",
    "evaluate(tokenized_data, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7504 [00:00<?, ? examples/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1 : 13.636363261363647\n",
      "rouge-2 : 8.510637917609795\n",
      "rouge-l : 4.545454170454576\n",
      "\n",
      "rouge-1 : 0.0\n",
      "rouge-2 : 0.0\n",
      "rouge-l : 0.0\n",
      "\n",
      "rouge-1 : 44.44444395833333\n",
      "rouge-2 : 5.7142852244898386\n",
      "rouge-l : 44.44444395833333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = prepare_data(2)\n",
    "evaluate(tokenized_data, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
