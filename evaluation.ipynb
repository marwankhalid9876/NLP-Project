{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import numpy as np\n",
    "s_output = load('S_outputs_text.joblib')\n",
    "l_output = load('L_outputs_text.joblib')\n",
    "output = load('outputs_text.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = 'moussaKam/AraBART'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def tokenize(df):\n",
    "    model_input = [row for row in df['text']]\n",
    "    model_input = tokenizer(model_input, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(df['summary'], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_input['labels'] = labels['input_ids']\n",
    "    return model_input\n",
    "\n",
    "\n",
    "column_name = ['cleaned_text', 'Lemmatized_Text', 'Stemmed_Text']\n",
    "\n",
    "def prepare_data(column_index=0):\n",
    "\n",
    "    train_inputs = pd.read_csv('../preprocessed_data/train_inputs.csv')\n",
    "    test_inputs = pd.read_csv('../preprocessed_data/test_inputs.csv')\n",
    "    validation_inputs = pd.read_csv('../preprocessed_data/validation_inputs.csv')\n",
    "\n",
    "    train_labels = pd.read_csv('../preprocessed_data/train_labels.csv')\n",
    "    test_labels = pd.read_csv('../preprocessed_data/test_labels.csv')\n",
    "    validation_labels = pd.read_csv('../preprocessed_data/validation_labels.csv')\n",
    "\n",
    "    train_size = round(train_labels.shape[0] * 0.2)\n",
    "    test_size = round(test_labels.shape[0] * 0.2)\n",
    "    val_size = round(validation_labels.shape[0] * 0.2)\n",
    "\n",
    "    train_inputs = train_inputs.iloc[:train_size]\n",
    "    train_labels = train_labels.iloc[:train_size]\n",
    "\n",
    "    test_inputs = test_inputs.iloc[:test_size]\n",
    "    test_labels = test_labels.iloc[:test_size]\n",
    "\n",
    "    validation_inputs = validation_inputs.iloc[:val_size]\n",
    "    validation_labels = validation_labels.iloc[:val_size]\n",
    "    df_train_x = train_inputs.copy().rename(columns={column_name[column_index]: 'text'})\n",
    "    df_train_y = train_labels.copy().rename(columns={'cleaned_text': 'summary'})\n",
    "    df_train = pd.concat([df_train_x['text'], df_train_y['summary']], axis=1)\n",
    "\n",
    "    df_test_x = test_inputs.copy().rename(columns={column_name[column_index]: 'text'})\n",
    "    df_test_y = test_labels.copy().rename(columns={'cleaned_text': 'summary'})\n",
    "    df_test = pd.concat([df_test_x['text'], df_test_y['summary']], axis=1)\n",
    "\n",
    "    df_val_x = validation_inputs.copy().rename(columns={column_name[column_index]: 'text'})\n",
    "    df_val_y = validation_labels.copy().rename(columns={'cleaned_text': 'summary'})\n",
    "    df_validation = pd.concat([df_val_x['text'], df_val_y['summary']], axis=1)\n",
    "\n",
    "    train_dataset = Dataset.from_dict(df_train)\n",
    "    test_dataset = Dataset.from_dict(df_test)\n",
    "    validation_dataset = Dataset.from_dict(df_validation)\n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"validation\": validation_dataset})\n",
    "    tokenized_data = dataset.map(tokenize, batched=True)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "outputs = [output, l_output, s_output]\n",
    "\n",
    "def evaluate(tokenized_data, index=0):\n",
    "    rouge_1_sum = 0.0\n",
    "    rouge_2_sum = 0.0\n",
    "    rouge_l_sum = 0.0\n",
    "\n",
    "    score = Rouge().get_scores(outputs[index], tokenized_data['test']['summary'])\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(score)):\n",
    "        rouge_1 = score[i]['rouge-1']['f']\n",
    "        rouge_2 = score[i]['rouge-2']['f']\n",
    "        rouge_l = score[i]['rouge-l']['f']\n",
    "\n",
    "        rouge_1_sum += rouge_1\n",
    "        rouge_2_sum += rouge_2\n",
    "        rouge_l_sum += rouge_l\n",
    "    \n",
    "    avg_rouge_1 = rouge_1_sum / len(score)\n",
    "    avg_rouge_2 = rouge_2_sum / len(score)\n",
    "    avg_rouge_l = rouge_l_sum / len(score)\n",
    "\n",
    "    print(\"Average Rouge Scores:\")\n",
    "    print(\"rouge-1: {:.2f}\".format(avg_rouge_1 * 100))\n",
    "    print(\"rouge-2: {:.2f}\".format(avg_rouge_2 * 100))\n",
    "    print(\"rouge-l: {:.2f}\".format(avg_rouge_l * 100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7504 [00:00<?, ? examples/s]/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge Scores:\n",
      "rouge-1: 23.48\n",
      "rouge-2: 10.15\n",
      "rouge-l: 21.02\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = prepare_data(0)\n",
    "evaluate(tokenized_data, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7504 [00:00<?, ? examples/s]/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge Scores:\n",
      "rouge-1: 21.16\n",
      "rouge-2: 8.26\n",
      "rouge-l: 18.77\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = prepare_data(1)\n",
    "evaluate(tokenized_data, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7504 [00:00<?, ? examples/s]/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge Scores:\n",
      "rouge-1: 16.19\n",
      "rouge-2: 5.36\n",
      "rouge-l: 14.38\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = prepare_data(2)\n",
    "evaluate(tokenized_data, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
