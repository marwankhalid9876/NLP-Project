{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_Text</th>\n",
       "      <th>Punctuation_Counts_before</th>\n",
       "      <th>Diacritic_Counts</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Stemmed_Text</th>\n",
       "      <th>Lemmatized_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>وكان الرئيس الاوكراني المؤقت  الكسندر تورتشينو...</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>وكان الرئيس الاوكراني المؤقت الكسندر تورتشينوف...</td>\n",
       "      <td>وكان رئس وكر ؤقت كسندر تورتشينوف امر سحب قوت ا...</td>\n",
       "      <td>كان رئيس الاوكراني المؤقت الكسندر تورتشينوف مر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بحلول عام 2050 ستحتاج مصر الي 21 مليار متر مكع...</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>بحلول عام 2050 ستحتاج مصر الي 21 مليار متر مكع...</td>\n",
       "      <td>حلل عام 2050 حاج مصر الي 21 لير متر كعب حصت حل...</td>\n",
       "      <td>حلول عام 2050 احتاج مصر ال 21 مليار متر مكعب ح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>وذكرت وكاله الانباء المحليه  جي ان اس  ان جماع...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>وذكرت وكاله الانباء المحليه جي ان اس ان جماعه ...</td>\n",
       "      <td>ذكر وكل باء حله جي ان اس ان جمع جيش حمد تشدد ا...</td>\n",
       "      <td>ذكر كال الانباء المحليه جي ان اس ان جماع جيش م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ووقع اختياره علي واد عمقه 800 متر محاط بثماني ...</td>\n",
       "      <td>378</td>\n",
       "      <td>15</td>\n",
       "      <td>ووقع اختياره علي واد عمقه 800 متر محاط بثماني ...</td>\n",
       "      <td>وقع خير علي واد عمق 800 متر حاط بثم قمم حده تش...</td>\n",
       "      <td>وقع اختيار علي وادي عمق 800 متر محاط بثماني قم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مسلح حوثي في اب  وقال المصدر ان المسلحين الحوث...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>مسلح حوثي اب وقال المصدر ان المسلحين الحوثيين ...</td>\n",
       "      <td>سلح حوث اب وقل صدر ان سلح حوث هجم شطء حرك همي ...</td>\n",
       "      <td>مسلح حوثي اب قال مصدر ان مسلح الحوثيين هاجم نش...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_Text  \\\n",
       "0  وكان الرئيس الاوكراني المؤقت  الكسندر تورتشينو...   \n",
       "1  بحلول عام 2050 ستحتاج مصر الي 21 مليار متر مكع...   \n",
       "2  وذكرت وكاله الانباء المحليه  جي ان اس  ان جماع...   \n",
       "3  ووقع اختياره علي واد عمقه 800 متر محاط بثماني ...   \n",
       "4  مسلح حوثي في اب  وقال المصدر ان المسلحين الحوث...   \n",
       "\n",
       "   Punctuation_Counts_before  Diacritic_Counts  \\\n",
       "0                        146                 0   \n",
       "1                         74                 1   \n",
       "2                         38                 0   \n",
       "3                        378                15   \n",
       "4                         26                 3   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  وكان الرئيس الاوكراني المؤقت الكسندر تورتشينوف...   \n",
       "1  بحلول عام 2050 ستحتاج مصر الي 21 مليار متر مكع...   \n",
       "2  وذكرت وكاله الانباء المحليه جي ان اس ان جماعه ...   \n",
       "3  ووقع اختياره علي واد عمقه 800 متر محاط بثماني ...   \n",
       "4  مسلح حوثي اب وقال المصدر ان المسلحين الحوثيين ...   \n",
       "\n",
       "                                        Stemmed_Text  \\\n",
       "0  وكان رئس وكر ؤقت كسندر تورتشينوف امر سحب قوت ا...   \n",
       "1  حلل عام 2050 حاج مصر الي 21 لير متر كعب حصت حل...   \n",
       "2  ذكر وكل باء حله جي ان اس ان جمع جيش حمد تشدد ا...   \n",
       "3  وقع خير علي واد عمق 800 متر حاط بثم قمم حده تش...   \n",
       "4  سلح حوث اب وقل صدر ان سلح حوث هجم شطء حرك همي ...   \n",
       "\n",
       "                                     Lemmatized_Text  \n",
       "0  كان رئيس الاوكراني المؤقت الكسندر تورتشينوف مر...  \n",
       "1  حلول عام 2050 احتاج مصر ال 21 مليار متر مكعب ح...  \n",
       "2  ذكر كال الانباء المحليه جي ان اس ان جماع جيش م...  \n",
       "3  وقع اختيار علي وادي عمق 800 متر محاط بثماني قم...  \n",
       "4  مسلح حوثي اب قال مصدر ان مسلح الحوثيين هاجم نش...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs = pd.read_csv('../preprocessed_data/train_inputs.csv')\n",
    "train_inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_Text</th>\n",
       "      <th>Punctuation_Counts_before</th>\n",
       "      <th>Diacritic_Counts</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بدأت القوات الأوكرانيه الانسحاب من شبه جزيره ا...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>بدأت القوات الأوكرانيه الانسحاب من شبه جزيره ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>هل سيتم تغيير العباره الشهيره للمؤرخ اليوناني...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>هل سيتم تغيير العباره الشهيره للمؤرخ اليوناني...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>قالت الشرطه في القطاع الهندي من إقليم كشمير إن...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>قالت الشرطه في القطاع الهندي من إقليم كشمير إن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>في عام 816  تجول راهب يدعي كوكاي  في المنحدرات...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>في عام 816  تجول راهب يدعي كوكاي  في المنحدرات...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>أكد مصدر في  الحراك التهامي  لأبناء محافظه الح...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>أكد مصدر في  الحراك التهامي  لأبناء محافظه الح...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_Text  \\\n",
       "0  بدأت القوات الأوكرانيه الانسحاب من شبه جزيره ا...   \n",
       "1   هل سيتم تغيير العباره الشهيره للمؤرخ اليوناني...   \n",
       "2  قالت الشرطه في القطاع الهندي من إقليم كشمير إن...   \n",
       "3  في عام 816  تجول راهب يدعي كوكاي  في المنحدرات...   \n",
       "4  أكد مصدر في  الحراك التهامي  لأبناء محافظه الح...   \n",
       "\n",
       "   Punctuation_Counts_before  Diacritic_Counts  \\\n",
       "0                          3                 0   \n",
       "1                          8                 0   \n",
       "2                          3                 0   \n",
       "3                          9                 0   \n",
       "4                          3                 0   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  بدأت القوات الأوكرانيه الانسحاب من شبه جزيره ا...  \n",
       "1   هل سيتم تغيير العباره الشهيره للمؤرخ اليوناني...  \n",
       "2  قالت الشرطه في القطاع الهندي من إقليم كشمير إن...  \n",
       "3  في عام 816  تجول راهب يدعي كوكاي  في المنحدرات...  \n",
       "4  أكد مصدر في  الحراك التهامي  لأبناء محافظه الح...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv('../preprocessed_data/train_labels.csv')\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>وكان الرئيس الاوكراني المؤقت الكسندر تورتشينوف...</td>\n",
       "      <td>بدأت القوات الأوكرانيه الانسحاب من شبه جزيره ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بحلول عام 2050 ستحتاج مصر الي 21 مليار متر مكع...</td>\n",
       "      <td>هل سيتم تغيير العباره الشهيره للمؤرخ اليوناني...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>وذكرت وكاله الانباء المحليه جي ان اس ان جماعه ...</td>\n",
       "      <td>قالت الشرطه في القطاع الهندي من إقليم كشمير إن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ووقع اختياره علي واد عمقه 800 متر محاط بثماني ...</td>\n",
       "      <td>في عام 816  تجول راهب يدعي كوكاي  في المنحدرات...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مسلح حوثي اب وقال المصدر ان المسلحين الحوثيين ...</td>\n",
       "      <td>أكد مصدر في  الحراك التهامي  لأبناء محافظه الح...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  وكان الرئيس الاوكراني المؤقت الكسندر تورتشينوف...   \n",
       "1  بحلول عام 2050 ستحتاج مصر الي 21 مليار متر مكع...   \n",
       "2  وذكرت وكاله الانباء المحليه جي ان اس ان جماعه ...   \n",
       "3  ووقع اختياره علي واد عمقه 800 متر محاط بثماني ...   \n",
       "4  مسلح حوثي اب وقال المصدر ان المسلحين الحوثيين ...   \n",
       "\n",
       "                                             summary  \n",
       "0  بدأت القوات الأوكرانيه الانسحاب من شبه جزيره ا...  \n",
       "1   هل سيتم تغيير العباره الشهيره للمؤرخ اليوناني...  \n",
       "2  قالت الشرطه في القطاع الهندي من إقليم كشمير إن...  \n",
       "3  في عام 816  تجول راهب يدعي كوكاي  في المنحدرات...  \n",
       "4  أكد مصدر في  الحراك التهامي  لأبناء محافظه الح...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = train_inputs.copy().rename(columns={'cleaned_text': 'text'})\n",
    "df_y = train_labels.copy().rename(columns={'cleaned_text': 'summary'})\n",
    "df = pd.concat([df_x['text'], df_y['summary']], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "model = 'moussaKam/AraBART'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "# Tokenizes the input dataframe to match the used AraBART model\n",
    "# Returns a dictionary {input_ids, attention_mask, summary}\n",
    "def tokenize(df):\n",
    "    model_input = [row for row in df['text']]\n",
    "    model_input = tokenizer(model_input, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(df['summary'], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_input['summary'] = labels['input_ids']\n",
    "    return model_input\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different lengths??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/37519 [00:00<?, ? examples/s]/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "tokenized_data = train_dataset.map(tokenize, batched=True)\n",
    "#tokenized_data = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "pre_trained_model = AutoModelForSeq2SeqLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "model_name = 'AraBART'\n",
    "arguments = Seq2SeqTrainingArguments(\n",
    "    model_name,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=pre_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    pre_trained_model,\n",
    "    arguments,\n",
    "    train_dataset=tokenized_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: summary, text. If summary, text are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 37519\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14070\n",
      "  Number of trainable parameters = 139221504\n",
      "  0%|          | 0/14070 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values,encoder_last_hidden_state. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2507\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2508\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2511\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2552\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m outputs:\n\u001b[0;32m-> 2553\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2554\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(outputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. For reference, the inputs it received are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2556\u001b[0m         )\n\u001b[1;32m   2557\u001b[0m     \u001b[39m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2558\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values,encoder_last_hidden_state. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
